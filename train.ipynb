{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from models.Bigram import BigramLanguageModel\n",
    "from models.GPT import GPTLanguageModel\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3fc61ace70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'gpt'\n",
    "# Data Params\n",
    "TRAIN_PERCENTAGE = 0.9\n",
    "CONTEXT_LENGTH = 256\n",
    "\n",
    "\n",
    "# Training Params\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 3e-4\n",
    "MAX_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 65\n",
      "Size of dataset: 1115394\n"
     ]
    }
   ],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print('Number of unique characters:', vocab_size)\n",
    "print('Size of dataset:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  1003854\n",
      "Val size:  111540\n"
     ]
    }
   ],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(TRAIN_PERCENTAGE*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print('Train size: ',len(train_data))\n",
    "print('Val size: ',len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # ix refers to the index of the first character in the context\n",
    "    # Contains BATCH_SIZE random integers between 0 and len(data) - CONTEXT_LENGTH\n",
    "    ix = torch.randint(len(data) - CONTEXT_LENGTH, (BATCH_SIZE,))\n",
    "    \n",
    "    # x and y are now BATCH_SIZE x CONTEXT_LENGTH tensors\n",
    "    # y is the same as x but shifted one character to the right\n",
    "    x = torch.stack([data[i:i+CONTEXT_LENGTH] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+CONTEXT_LENGTH+1] for i in ix])\n",
    "   \n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT model initialized with 12.558401 M parameters\n"
     ]
    }
   ],
   "source": [
    "if MODEL_TYPE.lower() == 'gpt':\n",
    "    model = GPTLanguageModel(vocab_size, CONTEXT_LENGTH, device)\n",
    "elif MODEL_TYPE.lower() == 'bigram':\n",
    "    model = BigramLanguageModel(vocab_size, device)\n",
    "else:\n",
    "    raise ValueError('Unknown model version')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(eval_steps=100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_steps)\n",
    "        for k in range(eval_steps):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(MAX_ITER):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % (MAX_ITER/10) == 0:\n",
    "        losses = estimate_loss(10)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' you requthring\\nAnd now choose noble tto add butcheres it.\\nIn this difference, my son and this day,\\nLove it it would do what wash, if she see\\nOthe worthy whom contented nave hand\\nO nuch, giving to peac'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_text = ' '\n",
    "decode(model.predict(encode(start_text), max_new_tokens=200).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_charset(charset, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for char in charset:\n",
    "            f.write(char)\n",
    "                \n",
    "save_charset(chars, 'charset.txt')\n",
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
